{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision as vision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger():\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    # handler1\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # handler2\n",
    "    handler2 = FileHandler(filename=dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")+\".log\")\n",
    "    handler2.setFormatter(Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "    # addHandler\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Csv Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv  sample_submission.csv  test  train\ttrain.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../input/imet-2019-fgvc6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>attribute_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000483014d91860</td>\n",
       "      <td>147 616 813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000fe2e667721fe</td>\n",
       "      <td>51 616 734 813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001614cb89646ee</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10041eb49b297c08</td>\n",
       "      <td>51 671 698 813 1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100501c227f8beea</td>\n",
       "      <td>13 404 492 903 1093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id        attribute_ids\n",
       "0  1000483014d91860          147 616 813\n",
       "1  1000fe2e667721fe       51 616 734 813\n",
       "2  1001614cb89646ee                  776\n",
       "3  10041eb49b297c08  51 671 698 813 1092\n",
       "4  100501c227f8beea  13 404 492 903 1093"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"../input/imet-2019-fgvc6/labels.csv\")\n",
    "train = pd.read_csv(\"../input/imet-2019-fgvc6/train.csv\")\n",
    "sample = pd.read_csv(\"../input/imet-2019-fgvc6/sample_submission.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_id</th>\n",
       "      <th>attribute_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>culture::abruzzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>culture::achaemenid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>culture::aegean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>culture::afghan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>culture::after british</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attribute_id          attribute_name\n",
       "0             0        culture::abruzzi\n",
       "1             1     culture::achaemenid\n",
       "2             2         culture::aegean\n",
       "3             3         culture::afghan\n",
       "4             4  culture::after british"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels.attribute_id.nunique())\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '../input/pytorch-pretrained-image-models/*': No such file or directory\r\n",
      "2019-04-26-07-14-00.log  __notebook__.ipynb  __output__.json\r\n"
     ]
    }
   ],
   "source": [
    "!cp ../input/pytorch-pretrained-image-models/* ./\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_transforms[\"test\"] = data_transforms[\"val\"]\n",
    "\n",
    "# Without KFold, We won't use data_transforms[\"val\"] this time...\n",
    "# We will use data_transforms[\"train\"] for \"..input/train\" folders, data_transforms[\"test\"] for \"..input/test\" folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMetDataset(data.Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', index=[], device=\"cuda:0\",\n",
    "                 transform=transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), \n",
    "                 labels_dict={}):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.index = index\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        self.labels_dict = labels_dict\n",
    "        if self.datatype == 'train' or self.datatype == 'val':\n",
    "            if self.index==[]:\n",
    "                self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "                self.labels = [labels_dict[i.split('.')[0]] for i in self.image_files_list]\n",
    "            else:\n",
    "                self.image_files_list = np.array([s for s in os.listdir(datafolder)])[index]\n",
    "                self.labels = [labels_dict[i.split('.')[0]] for i in self.image_files_list]\n",
    "        else:\n",
    "            self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "            self.labels = [0 for _ in range(len(self.image_files_list))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n",
    "        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n",
    "        image = Image.open(img_name)\n",
    "        image = self.transform(image)\n",
    "        img_name_short = self.image_files_list[idx].split('.')[0]\n",
    "        if self.datatype == 'train' or self.datatype == 'val':\n",
    "            label = self.labels_dict[img_name_short]\n",
    "            # Target has some labels, So we need to change labels to tensor\n",
    "            label_tensor = torch.zeros((1, 1103))\n",
    "            for i in label:\n",
    "                label_tensor[0, int(i)] = 1\n",
    "            label_tensor = label_tensor.to(self.device)\n",
    "            return image, label_tensor\n",
    "        else:\n",
    "            label = 0\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, logger, loss_fn, train_batch=32, valid_batch=128, seed=1234, \n",
    "                 device=\"cuda:0\", model_name=\"best_model\"):\n",
    "        self.model = model\n",
    "        self.logger = logger\n",
    "        self.loss_fn = loss_fn.to(device)\n",
    "        self.train_batch = train_batch\n",
    "        self.valid_batch = valid_batch\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def fit(self, tr_idx, val_idx, img_class_dict, n_epochs=10, num_workers=0):\n",
    "        # train loader & valid loader\n",
    "        train_dataset = IMetDataset(datafolder='../input/imet-2019-fgvc6/train/', \n",
    "                                    datatype='train', \n",
    "                                    index=tr_idx, \n",
    "                                    transform=data_transforms[\"train\"], \n",
    "                                    labels_dict=img_class_dict)\n",
    "        train_loader = data.DataLoader(train_dataset, \n",
    "                                       batch_size=self.train_batch,\n",
    "                                       shuffle=True)\n",
    "\n",
    "        valid_dataset = IMetDataset(datafolder='../input/imet-2019-fgvc6/train/', \n",
    "                                    datatype='val', \n",
    "                                    index=val_idx, \n",
    "                                    transform=data_transforms[\"val\"], \n",
    "                                    labels_dict=img_class_dict)\n",
    "        valid_loader = data.DataLoader(valid_dataset,\n",
    "                                       batch_size=self.valid_batch,\n",
    "                                       shuffle=False)\n",
    "        # model\n",
    "        model = self.model\n",
    "        model.to(self.device)\n",
    "        # optimizer\n",
    "        optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "        # scheduler\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "        # set best_score\n",
    "        best_score = np.inf\n",
    "        # master_bar\n",
    "        mb = master_bar(range(n_epochs))\n",
    "        for epoch in mb:\n",
    "            model.train()\n",
    "            avg_loss = 0.0\n",
    "            train_losses = []\n",
    "            for i_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # pred & loss\n",
    "                y_pred = model(i_batch.cuda())\n",
    "                y_batch = y_batch.view(y_batch.size()[0], -1) # flatten y_batch\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                #self.logger.info(\"loss: {}\".format(loss))\n",
    "                # backward + optimize in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # avg_loss\n",
    "                avg_loss = loss.item()\n",
    "                train_losses.append(avg_loss)\n",
    "                #avg_loss += loss.item() / len(train_loader)\n",
    "                #self.logger.info(\"avg_loss: {:.8f}    loss.item(): {}    len(train_loader): {}\"\\\n",
    "                             #.format(avg_loss, loss.item(), len(train_loader)))\n",
    "            # val\n",
    "            valid_preds, valid_y, avg_val_loss = self.val(valid_loader, model)\n",
    "            scheduler.step()\n",
    "            # save best model\n",
    "            if best_score > avg_val_loss:\n",
    "                torch.save(model.state_dict(), self.model_name+\".pth\")\n",
    "                torch.save(optimizer.state_dict(), \"optimizer.pth\")\n",
    "                torch.save(scheduler.state_dict(), \"scheduler.pth\")\n",
    "                best_score = avg_val_loss\n",
    "            # logger\n",
    "            self.logger.info(\"Epoch {} / {} \".format(epoch+1, n_epochs))\n",
    "            #self.logger.info(\"avg_loss: {:.8f}    avg_val_loss: {:.8f}    best_avg_val_loss: {:.8f}\"\\\n",
    "                             #.format(avg_loss, avg_val_loss, best_score))\n",
    "            self.logger.info(\"avg_loss: {:.8f}    avg_val_loss: {:.8f}    best_avg_val_loss: {:.8f}\"\\\n",
    "                             .format(sum(train_losses[-100:])/len(train_losses[-100:]), avg_val_loss, best_score))\n",
    "        # output of best model      \n",
    "        model.load_state_dict(torch.load(self.model_name+\".pth\"))\n",
    "        # eval\n",
    "        valid_preds, valid_y, _ = self.val(valid_loader, model)\n",
    "        return valid_preds, valid_y\n",
    "    \n",
    "    def val(self, loader, model):\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(loader.dataset), 1103))\n",
    "        valid_y = np.zeros((len(loader.dataset), 1103))\n",
    "        avg_val_loss = 0.0\n",
    "        for i, (i_batch, y_batch) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(i_batch.cuda()).detach()\n",
    "                y_batch = y_batch.view(y_batch.size()[0], -1) # flatten y_batch\n",
    "                avg_val_loss += self.loss_fn(y_pred, y_batch).item() / len(loader)\n",
    "                if i_batch.size()[0]==self.valid_batch:\n",
    "                    valid_preds[i * self.valid_batch : (i+1) * self.valid_batch] = y_pred.cpu().numpy()\n",
    "                    valid_y[i * self.valid_batch : (i+1) * self.valid_batch] = y_batch.cpu().numpy()\n",
    "                else:\n",
    "                    valid_preds[len(loader.dataset) - i_batch.size()[0] : ] = y_pred.cpu().numpy()\n",
    "                    valid_y[len(loader.dataset) - i_batch.size()[0] : ] = y_batch.cpu().numpy()\n",
    "        return valid_preds, valid_y, avg_val_loss\n",
    "    \n",
    "    def predict(self, loader):\n",
    "        model = self.model\n",
    "        model.load_state_dict(torch.load(self.model_name+\".pth\"))\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        preds = np.zeros((len(loader.dataset), 1103))\n",
    "        for i, (i_batch, _) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(i_batch.cuda()).detach()\n",
    "                if i_batch.size()[0]==self.valid_batch:\n",
    "                    preds[i * self.valid_batch : (i+1) * self.valid_batch] = y_pred.cpu().numpy()\n",
    "                else:\n",
    "                    preds[len(loader.dataset) - i_batch.size()[0] : ] = y_pred.cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /tmp/.torch/models/resnet18-5c106cde.pth\n",
      "46827520it [00:00, 108387067.01it/s]\n"
     ]
    }
   ],
   "source": [
    "model_conv = models.resnet18(pretrained='imagenet')\n",
    "#for i, param in model_conv.named_parameters():\n",
    "    #param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "#model_conv.fc = nn.Linear(num_ftrs, 1103)\n",
    "model_conv.fc = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2層のMLP\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, num_ftrs),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_ftrs),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(num_ftrs, 1103)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なCNN\n",
    "net = nn.Sequential(\n",
    "    model_conv,\n",
    "    mlp,\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss(reduction=\"mean\")\n",
    "#loss_fn = FocalLoss()\n",
    "trainer = Trainer(net, logger, loss_fn, train_batch=32, valid_batch=128, seed=1234, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for validation\n",
    "tr, val = train_test_split(train.id, test_size=0.2, random_state=1234)\n",
    "tr, val = tr.index, val.index\n",
    "#tr, val = list(tr), list(val)\n",
    "# X_train, y_train, X_val, y_val\n",
    "img_class_dict = {k:v for k, v in zip(train.id, train.attribute_ids.map(lambda x: x.split()).values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.00% [1/20 19:52<6:17:40]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='690' class='' max='2731', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      25.27% [690/2731 04:03<12:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-26 07:34:00,235     INFO Epoch 1 / 20 \n",
      "2019-04-26 07:34:00,237     INFO avg_loss: 0.01563551    avg_val_loss: 0.01544282    best_avg_val_loss: 0.01544282\n"
     ]
    }
   ],
   "source": [
    "valid_preds, valid_y = trainer.fit(tr, val, img_class_dict, n_epochs=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post process - threshold search -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I used sigmoid for the activation, I've got the 1103 probability output for each data row.\n",
    "\n",
    "I need to decide threshold for this.There are two ways to deal with this.\n",
    "\n",
    "- Class-wise threshold search\n",
    "  - Takes some time but it's natural.\n",
    "- One threshold for all the class\n",
    "  - Low cost way.\n",
    "\n",
    "**UPDATE**\n",
    "I will use the first -> second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_pred, y_true):\n",
    "    score = []\n",
    "    candidates = np.arange(0, 0.4, 0.01)\n",
    "    for th in progress_bar(candidates):\n",
    "        yp = (y_pred > th).astype(int)\n",
    "        score.append(fbeta_score(y_pred=yp, y_true=y_true, beta=2, average=\"samples\"))\n",
    "    score = np.array(score)\n",
    "    pm = score.argmax()\n",
    "    best_th, best_score = candidates[pm], score[pm]\n",
    "    plt.plot(candidates, score)\n",
    "    plt.vlines(x=best_th, ymin=score.min(), ymax=score.max())\n",
    "    plt.text(best_th+0.03, best_score-0.01, f'$F_{2}=${best_score:.3f}', fontsize=14);\n",
    "    plt.show()\n",
    "    return best_th, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny = train.attribute_ids.map(lambda x: x.split()).values\\ny_true = np.zeros((train.shape[0], 1103)).astype(int)\\nfor i, row in enumerate(y):\\n    for idx in row:\\n        y_true[i, int(idx)] = 1\\n        '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y = train.attribute_ids.map(lambda x: x.split()).values\n",
    "y_true = np.zeros((train.shape[0], 1103)).astype(int)\n",
    "for i, row in enumerate(y):\n",
    "    for idx in row:\n",
    "        y_true[i, int(idx)] = 1\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='40' class='' max='40', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [40/40 01:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_threshold, best_score = threshold_search(valid_preds, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_preds = trainer.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = (test_preds > best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprediction = []\\nfor i in range(preds.shape[0]):\\n    pred1 = np.argwhere(preds[i] == 1.0).reshape(-1).tolist()\\n    pred_str = \" \".join(list(map(str, pred1)))\\n    prediction.append(pred_str)\\n    \\nsample.attribute_ids = prediction\\nsample.to_csv(\"submission.csv\", index=False)\\nsample.head()\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "prediction = []\n",
    "for i in range(preds.shape[0]):\n",
    "    pred1 = np.argwhere(preds[i] == 1.0).reshape(-1).tolist()\n",
    "    pred_str = \" \".join(list(map(str, pred1)))\n",
    "    prediction.append(pred_str)\n",
    "    \n",
    "sample.attribute_ids = prediction\n",
    "sample.to_csv(\"submission.csv\", index=False)\n",
    "sample.head()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
